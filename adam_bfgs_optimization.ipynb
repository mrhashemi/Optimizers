{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of Adam optimizar as a class is obtained from:\n",
    "\n",
    "https://towardsdatascience.com/how-to-implement-an-adam-optimizer-from-scratch-76e7b217f1cc"
    "https://github.com/enochkan/building-from-scratch/blob/main/optimizers/adam-optimizer-from-scratch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptim():\n",
    "    def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.m_dw, self.v_dw = 0, 0\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_updated_weights(self, t, w, dw, eta):\n",
    "        ## momentum beta 1\n",
    "        # *** weights *** #\n",
    "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
    "\n",
    "        ## rms beta 2\n",
    "        # *** weights *** #\n",
    "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
    "\n",
    "        ## correction\n",
    "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
    "        v_dw_corr = self.v_dw/(1-self.beta2**t)\n",
    "\n",
    "        ## updating the weights\n",
    "        w = w - eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of the BFGS algorithm is based on:\n",
    "\n",
    "https://github.com/trsav/bfgs/blob/master/BFGS.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFGSOptim():\n",
    "    def __init__(self, w0, grad_error=1.0e-6):\n",
    "        \"\"\" If the size of x is provided, more objects can be initialized! \"\"\"\n",
    "        self.a = 1.0\n",
    "        self.c1 = 1.0e-4 \n",
    "        self.c2 = 0.9\n",
    "\n",
    "        self.d = w0.size \n",
    "        self.H = np.eye(self.d)\n",
    "        self.w = w0[:] # copy the array (not a reference)\n",
    "        self.w_new = w0[:] # copy the array (not a reference)\n",
    "        self.it = 2\n",
    "\n",
    "        self.error_tolerance = grad_error\n",
    "\n",
    "\n",
    "    def line_search(self, func, grad_func):\n",
    "        self.a = 1.0\n",
    "        self.grad = grad_func(self.w)\n",
    "        self.p = -self.H@self.grad # search direction (Newton Method)\n",
    "        f = func(self.w)\n",
    "        self.w_new = self.w + self.a * self.p\n",
    "        self.grad_new = grad_func(self.w_new)\n",
    "\n",
    "        while func(self.w_new) >= f + (self.c1*self.a*self.grad.T@self.p) or self.grad_new.T@self.p <= self.c2*self.grad.T@self.p: \n",
    "            self.a *= 0.5\n",
    "            self.w_new = self.w + self.a * self.p\n",
    "            self.grad_new = grad_func(self.w_new)\n",
    "\n",
    "        return self.a\n",
    "\n",
    "\n",
    "    def get_updated_weights(self, func, grad_func, w0):\n",
    "        self.w = w0[:]\n",
    "        self.it += 1\n",
    "        self.a = self.line_search(func, grad_func) # line search \n",
    "        \n",
    "        s = self.a * self.p\n",
    "        s = np.array([s])\n",
    "        s = np.reshape(s,(self.d,1))\n",
    "        \n",
    "        # self.w_new = self.w + self.a * self.p # done in line search function \n",
    "        # self.grad_new = grad_func(self.w_new) # done in line search function         \n",
    "        y = self.grad_new - self.grad\n",
    "        y = np.array([y])\n",
    "        y = np.reshape(y,(self.d,1))\n",
    "        \n",
    "        r = 1/(y.T@s)\n",
    "        li = (np.eye(self.d)-(r*((s@(y.T)))))\n",
    "        ri = (np.eye(self.d)-(r*((y@(s.T)))))\n",
    "\n",
    "        hess_inter = li@self.H@ri\n",
    "        self.H = hess_inter + (r*((s@(s.T)))) # BFGS Update\n",
    "        self.grad = self.grad_new[:] \n",
    "        # self.w = self.w_new[:]\n",
    "    \n",
    "        return self.w_new\n",
    "\n",
    "    def check_convergence(self):\n",
    "        if_converged = False\n",
    "        if np.linalg.norm(self.grad) <= self.error_tolerance:\n",
    "            if_converged = True\n",
    "\n",
    "        return if_converged\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    '''\n",
    "    FUNCTION TO BE OPTIMISED\n",
    "    '''\n",
    "    d = x.size\n",
    "    return sum(100*(x[i+1]-x[i]**2)**2 + (x[i]-1)**2 for i in range(d-1))\n",
    "\n",
    "def grad(f,x): \n",
    "    '''\n",
    "    CENTRAL FINITE DIFFERENCE CALCULATION\n",
    "    '''\n",
    "    h = np.cbrt(np.finfo(float).eps)\n",
    "    # d = len(x)\n",
    "    nabla = np.zeros_like(x)\n",
    "    d = x.size\n",
    "    for i in range(d): \n",
    "        x_for = np.copy(x) \n",
    "        x_back = np.copy(x)\n",
    "        x_for[i] += h \n",
    "        x_back[i] -= h \n",
    "        nabla[i] = (f(x_for) - f(x_back))/(2*h) \n",
    "    return nabla \n",
    "\n",
    "def grad_f(x): \n",
    "    return grad(f,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38644555 0.07275717]\n",
      "0.962944602340883\n",
      "[0.99999999 0.99999999]\n",
      "5.3139387276875856e-17\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-1.2,-1.2])\n",
    "\n",
    "adam_optimizer1 = AdamOptim()\n",
    "learning_rate = 1.0e-1\n",
    "for iter in range(50):\n",
    "    nabla = grad(f,x)\n",
    "    x = adam_optimizer1.get_updated_weights(iter+1, x, nabla, learning_rate)\n",
    "\n",
    "print(x)\n",
    "print(f(x))\n",
    "\n",
    "bfgs_optimizer = BFGSOptim(x)\n",
    "converged = False\n",
    "while (not converged and bfgs_optimizer.it < 30):\n",
    "    x = bfgs_optimizer.get_updated_weights(f, grad_f, x)\n",
    "    converged = bfgs_optimizer.check_convergence()\n",
    "\n",
    "print(x)\n",
    "print(f(x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
